{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with the single-scale input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xin/miniconda3/envs/venv-tibetwater/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from notebooks import config\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.metric import oa_binary, miou_binary\n",
    "from utils.imgShow import imsShow\n",
    "from dataloader.preprocess import read_normalize\n",
    "from model.seg_model.deeplabv3plus import deeplabv3plus\n",
    "from model.seg_model.hrnet import hrnet\n",
    "from dataloader.parallel_loader import threads_scene_dset\n",
    "from dataloader.loader import patch_tensor_dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f06bbeb32d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------- Device --------------- #\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# ---------- setting ------- #\n",
    "torch.manual_seed(999)   # make the trianing replicable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xin/Developer-luo/Monthly-Surface-Water-in-Tibet/data/dset/s1_ascend_clean/scene01_s1as_pad.tif\n"
     ]
    }
   ],
   "source": [
    "# ---------- Data paths ----------- #\n",
    "paths_as = sorted(glob.glob(config.dir_as + '/*pad*'))\n",
    "paths_des = sorted(glob.glob(config.dir_des+'/*pad*'))\n",
    "paths_truth = sorted(glob.glob(config.dir_truth+'/*pad*'))\n",
    "paths_patch_val = sorted(glob.glob(config.dir_patch_val+'/*'))\n",
    "print(paths_as[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training data: 960\n",
      "size of val data: 350\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PROJ_LIB'] = '/home/xin/miniconda3/envs/venv-tibetwater/share/proj'\n",
    "\n",
    "'''--------- data loading --------'''\n",
    "### ------ 1. training data loading: from scenes path ----- ### \n",
    "### 1.1. training scenes selection (in the application, all scenes are used for training.)\n",
    "paths_tra_as, paths_tra_des, paths_tra_truth = [], [], []\n",
    "for tra_id in config.tra_ids:     ## select training scenes\n",
    "   as_name = 'scene'+tra_id+'_s1as_pad.tif'\n",
    "   des_name = 'scene'+tra_id+'_s1des_pad.tif'\n",
    "   truth_name = 'scene'+tra_id+'_wat_truth_pad.tif'\n",
    "   paths_tra_as.append(config.dir_as + '/' + as_name); \n",
    "   paths_tra_des.append(config.dir_des + '/' + des_name)\n",
    "   paths_tra_truth.append(config.dir_truth + '/' + truth_name)\n",
    "tra_scenes, tra_truths = read_normalize(paths_as=paths_tra_as, paths_des=paths_tra_des, \\\n",
    "                      paths_truth=paths_tra_truth, max_bands=config.s1_max, min_bands=config.s1_min)\n",
    "# ### !!!!extract either ascending or descending image.\n",
    "# tra_scene = [s[2:4] for s in tra_scenes]        ## [0:2] -> ascending; [2:4] -> descending\n",
    "\n",
    "### 1.2. training data loading\n",
    "tra_dset = threads_scene_dset(scene_list = tra_scenes, \\\n",
    "                              truth_list = tra_truths, \n",
    "                              transforms=config.transforms_tra, \n",
    "                              num_thread=30)\n",
    "\n",
    "### ----- 2. val data loading: from prepared validation patches ------ ###\n",
    "patch_list_val = [torch.load(path) for path in paths_patch_val]\n",
    "# !!!extract either ascending or descending image for validation\n",
    "# for i in range(len(patch_list_val)):\n",
    "#    for j in range(len(patch_list_val[0][0])):\n",
    "      # patch_list_val[i][0][j] = patch_list_val[i][0][j][2:4]   ## [0:2] -> ascending; [2:4] -> descending\n",
    "   #   patch_list_val[i][0][j][0:2] = 0   ## [0:2] is ascending; and [2:4] is descending\n",
    "\n",
    "val_dset = patch_tensor_dset(patch_pair_list = patch_list_val)\n",
    "\n",
    "### ------- print ------- ###\n",
    "print('size of training data:', tra_dset.__len__())\n",
    "print('size of val data:', val_dset.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_loader = torch.utils.data.DataLoader(tra_dset, \\\n",
    "                                batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------ Model ------------ ##\n",
    "## -------- 1. single scale -------\n",
    "# model = unet(num_bands=4, num_classes=2).to(device)\n",
    "# model = deeplabv3plus(num_bands=4, num_classes=2).to(device)\n",
    "# model = deeplabv3plus_imp(num_bands=4, num_classes=2).to(device)\n",
    "model = hrnet(num_bands=4, num_classes=2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\\\n",
    "                                            mode='min', factor=0.5, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------train step------'''\n",
    "def train_step(model, loss_fn, optimizer, x, y):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(x)\n",
    "    loss = loss_fn(pred, y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    miou = miou_binary(pred=pred, truth=y)\n",
    "    oa = oa_binary(pred=pred, truth=y)\n",
    "    return loss, miou, oa\n",
    "\n",
    "'''------validation step------'''\n",
    "def val_step(model, loss_fn, x, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y.float())\n",
    "    miou = miou_binary(pred=pred, truth=y)\n",
    "    oa = oa_binary(pred=pred, truth=y)\n",
    "    return loss, miou, oa\n",
    "\n",
    "'''------ train loops ------'''\n",
    "def train_loops(model, loss_fn, optimizer, tra_loader, val_loader, epoches, lr_scheduler):\n",
    "    size_tra_loader = len(tra_loader)\n",
    "    size_val_loader = len(val_loader)\n",
    "    tra_loss_loops, tra_miou_loops = [], []\n",
    "    val_loss_loops, val_miou_loops = [], []\n",
    "    for epoch in range(epoches):\n",
    "        start = time.time()\n",
    "        tra_loss, val_loss = 0, 0\n",
    "        tra_miou, val_miou = 0, 0\n",
    "        tra_oa, val_oa = 0, 0\n",
    "\n",
    "        '''----- 1. train the model -----'''\n",
    "        for x_batch, y_batch in tra_loader:\n",
    "            x_batch, y_batch = x_batch[2].to(device), y_batch.to(device)\n",
    "            y_batch = config.label_smooth(y_batch, 0.1)         ### --- label smoothing\n",
    "            loss, miou, oa = train_step(model=model, loss_fn=loss_fn, \n",
    "                                            optimizer=optimizer, x=x_batch, y=y_batch)\n",
    "            tra_loss += loss.item()\n",
    "            tra_miou += miou.item()\n",
    "            tra_oa += oa.item()\n",
    "        lr_scheduler.step(tra_loss)    # dynamic adjust learning rate\n",
    "\n",
    "        '''----- 2. validate the model -----'''\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch[2].to(device).to(dtype=torch.float32)   # fine scale\n",
    "            y_batch = y_batch.to(device).to(dtype=torch.float32)\n",
    "\n",
    "            loss, miou, oa = val_step(model=model, loss_fn=loss_fn, \\\n",
    "                                                        x=x_batch, y=y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_miou += miou.item()\n",
    "            val_oa += oa.item()\n",
    "\n",
    "        '''------ 3. print accuracy ------'''\n",
    "        tra_loss = tra_loss/size_tra_loader\n",
    "        val_loss = val_loss/size_val_loader\n",
    "        tra_miou = tra_miou/size_tra_loader\n",
    "        val_miou = val_miou/size_val_loader\n",
    "        tra_oa = tra_oa/size_tra_loader\n",
    "        val_oa = val_oa/size_val_loader\n",
    "        tra_loss_loops.append(tra_loss), tra_miou_loops.append(tra_miou)\n",
    "        val_loss_loops.append(val_loss), val_miou_loops.append(val_miou)\n",
    "\n",
    "        format = 'Ep{}: tra-> Loss:{:.3f},Oa:{:.3f},Miou:{:.3f}, val-> Loss:{:.3f},Oa:{:.3f},Miou:{:.3f},time:{:.1f}s'\n",
    "        print(format.format(epoch+1, tra_loss, tra_oa, tra_miou, val_loss, val_oa, val_miou, time.time()-start))\n",
    "\n",
    "        '''------- 4. visualize the result -------'''\n",
    "        if (epoch+1)%10 == 0:\n",
    "            model.eval()\n",
    "            sam_index = random.randrange(len(val_dset))\n",
    "            patches, truth = val_dset[sam_index]\n",
    "            patch = torch.unsqueeze(patches[2], 0).to(device).to(dtype=torch.float32)   # \n",
    "            truth = truth.to(device)\n",
    "            pred = model(patch)\n",
    "            patch = patch[0].to('cpu').detach().numpy().transpose(1,2,0)\n",
    "            pred = pred[0].to('cpu').detach().numpy()\n",
    "            truth = truth.to('cpu').detach().numpy()\n",
    "            pred = np.where(pred>0.5, 1, 0)\n",
    "            patch_list = [patch, truth, pred]\n",
    "            # ---- plot result\n",
    "            plt.figure(figsize=(8,4))\n",
    "            patches_name = ['input','truth','pred']\n",
    "            clip_list = [2,0,0]\n",
    "            col_bands_list = [(2,1,0), (0,0,0), (0,0,0)]\n",
    "            imsShow(img_list=patch_list, img_name_list=patches_name, \\\n",
    "                                        clip_list=clip_list, color_bands_list=col_bands_list)\n",
    "            plt.show()\n",
    "\n",
    "    metrics = {'tra_loss':tra_loss_loops, 'tra_miou':tra_miou_loops, 'val_loss': val_loss_loops, 'val_miou': val_miou_loops}\n",
    "    return metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep1: tra-> Loss:1.335,Oa:0.505,Miou:0.280, val-> Loss:0.754,Oa:0.387,Miou:0.295,time:21.7s\n"
     ]
    }
   ],
   "source": [
    "metrics = train_loops(model=model,  \n",
    "                    loss_fn=config.loss_bce,  \n",
    "                    optimizer=optimizer,  \n",
    "                    tra_loader=tra_loader,  \n",
    "                    val_loader=val_loader,  \n",
    "                    epoches=100,   \n",
    "                    lr_scheduler=lr_scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and metrics saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'model_single_test_11'\n",
    "# # model save/load\n",
    "# model_weights = config.root_proj + '/model/pretrained/model_single_test/' + model_name + '_weights.pth'\n",
    "# torch.save(model.state_dict(), model_weights)\n",
    "# # model.load_state_dict(torch.load(model_weights))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## metrics saving\n",
    "# metrics_path = config.root_proj + '/model/pretrained/model_single_test/' + model_name + '_metrics.csv'\n",
    "# metrics_df = pd.DataFrame(metrics)\n",
    "# metrics_df.to_csv(metrics_path, index=False, sep=',')\n",
    "# # metrics_df = pd.read_csv(metrics_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-tibetwater",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "metadata": {
   "interpreter": {
    "hash": "6f5bb23da6bd6ab87296804a7ae062a565b497c650c7064ca78191dd29b49fd6"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "845e3c3edce3045d0fb8b6eb0e71ae925453319af75bc104ea4cd19886cbfe31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
