{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "from notebooks import config\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.metric import oa_binary, miou_binary\n",
    "from utils.plot_dset_one import plot_dset_one\n",
    "from dataloader.read_normalize import read_normalize\n",
    "from model.seg_model.unet import unet\n",
    "from model.seg_model.model_scales import unet_triple\n",
    "from model.seg_model.deeplabv3_plus import deeplabv3plus, deeplabv3plus_imp\n",
    "from dataloader.parallel_loader import threads_scene_dset\n",
    "from dataloader.loader import patch_tensor_dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Device --------------- #\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "# ---------- setting ------- #\n",
    "torch.manual_seed(999)   # make the trianing replicable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''--------- data loading --------'''\n",
    "### ------ 1. training data loading: from scenes path ----- ### \n",
    "scene_list, truth_list = read_normalize(paths_as=config.paths_as, \\\n",
    "                            paths_des=config.paths_des, paths_truth=config.paths_truth)\n",
    "tra_dset = threads_scene_dset(scene_list[0:15], \\\n",
    "                            truth_list[0:15], transforms=config.transforms_tra, num_thread=30)\n",
    "\n",
    "### ----- 2. val data loading: from prepared validation patches ------ ###\n",
    "patch_list_val = [torch.load(path) for path in config.paths_patch_val]\n",
    "val_dset = patch_tensor_dset(patch_pair_list = patch_list_val)\n",
    "\n",
    "### ------- print ------- ###\n",
    "print('size of training data:', tra_dset.__len__())\n",
    "print('size of val data:', val_dset.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_loader = torch.utils.data.DataLoader(tra_dset, \\\n",
    "                                batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dset, \\\n",
    "                                batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------ Model ------------ ##\n",
    "## -------- 1. single scale -------\n",
    "# model = unet(num_bands=4, num_classes=2).to(device)\n",
    "# model = deeplabv3plus(num_bands=4, num_classes=2).to(device)\n",
    "# model = deeplabv3plus_imp(num_bands=4, num_classes=2).to(device)\n",
    "## -------- 2. multiple scales -------\n",
    "model = unet_triple(num_bands=4, num_classes=2, \\\n",
    "                    scale_high=2048, scale_mid=512, scale_low=256).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\\\n",
    "                                        mode='min', factor=0.5, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------train step------'''\n",
    "def train_step(model, loss_fn, optimizer, x, y):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(x)\n",
    "    loss = loss_fn(pred, y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    miou = miou_binary(pred=pred, truth=y)\n",
    "    oa = oa_binary(pred=pred, truth=y)\n",
    "    return loss, miou, oa\n",
    "\n",
    "'''------validation step------'''\n",
    "def val_step(model, loss_fn, x, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y.float())\n",
    "    miou = miou_binary(pred=pred, truth=y)\n",
    "    oa = oa_binary(pred=pred, truth=y)\n",
    "    return loss, miou, oa\n",
    "\n",
    "'''------ train loops ------'''\n",
    "def train_loops(model, loss_fn, optimizer, tra_loader, val_loader, epoches, lr_scheduler):\n",
    "    size_tra_loader = len(tra_loader)\n",
    "    size_val_loader = len(val_loader)\n",
    "    tra_loss_loops, tra_miou_loops = [], []\n",
    "    val_loss_loops, val_miou_loops = [], []\n",
    "    for epoch in range(epoches):\n",
    "        start = time.time()\n",
    "        tra_loss, val_loss = 0, 0\n",
    "        tra_miou, val_miou = 0, 0\n",
    "        tra_oa, val_oa = 0, 0\n",
    "\n",
    "        '''----- 1. train the model -----'''\n",
    "        for x_batch, y_batch in tra_loader:\n",
    "            x_batch, y_batch = [batch.to(device) for batch in x_batch], y_batch.to(device)\n",
    "            # x_batch = x_batch[2]      # !!!note: x_batch[2] for single-scale model\n",
    "            loss, miou, oa = train_step(model=model, loss_fn=loss_fn, \n",
    "                                        optimizer=optimizer, x=x_batch, y=y_batch)\n",
    "            tra_loss += loss.item()\n",
    "            tra_miou += miou.item()\n",
    "            tra_oa += oa.item()\n",
    "        lr_scheduler.step(tra_loss)    # dynamic adjust learning rate\n",
    "\n",
    "        '''----- 2. validate the model -----'''\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch, y_batch = [batch.to(device) for batch in x_batch], y_batch.to(device)            \n",
    "            # x_batch = x_batch[2]          #!!!note: x_batch[2] for single-scale model\n",
    "            loss, miou, oa = val_step(model=model, loss_fn=loss_fn, \n",
    "                                        x=x_batch, y=y_batch)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_miou += miou.item()\n",
    "            val_oa += oa.item()\n",
    "\n",
    "        '''------ 3. print accuracy ------'''\n",
    "        tra_loss = tra_loss/size_tra_loader\n",
    "        val_loss = val_loss/size_val_loader\n",
    "        tra_miou = tra_miou/size_tra_loader\n",
    "        val_miou = val_miou/size_val_loader\n",
    "        tra_oa = tra_oa/size_tra_loader\n",
    "        val_oa = val_oa/size_val_loader\n",
    "        tra_loss_loops.append(tra_loss), tra_miou_loops.append(tra_miou)\n",
    "        val_loss_loops.append(val_loss), val_miou_loops.append(val_miou)\n",
    "\n",
    "        format = 'Ep{}: tra-> Loss:{:.3f},Oa:{:.2f},Miou:{:.2f}, val-> Loss:{:.2f},Oa:{:.2f},Miou:{:.2f},time:{:.0f}s'\n",
    "        print(format.format(epoch+1, tra_loss, tra_oa, tra_miou, val_loss, val_oa, val_miou, time.time()-start))\n",
    "\n",
    "        '''------- 4. visualize the result -------'''\n",
    "        if (epoch+1)%10 == 0:\n",
    "            model.eval()\n",
    "            sam_index = random.randrange(len(val_dset))\n",
    "            patches, truth = val_dset[sam_index]\n",
    "            patches = [torch.unsqueeze(patch, 0).to(device) for patch in patches]\n",
    "            truth = truth.to(device)\n",
    "            # pred = model(patches[2])       #!!!note: x[2] for single-scale model\n",
    "            pred = model(patches)            \n",
    "            patches = [patch[0].to('cpu').detach().numpy().transpose(1,2,0) for patch in patches]\n",
    "            pred = pred[0].to('cpu').detach().numpy()\n",
    "            truth = truth.to('cpu').detach().numpy()\n",
    "            plt.figure(figsize=(12,5))\n",
    "            plot_dset_one(inputs=patches, truth=truth, pre=pred, binary_out=True, weights=None)\n",
    "    metrics = {'tra_loss':tra_loss_loops, 'tra_miou':tra_miou_loops, 'val_loss': val_loss_loops, 'val_miou': val_miou_loops}\n",
    "    return metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_loops(model=model,  \n",
    "                    loss_fn=config.loss_bce,  \n",
    "                    optimizer=optimizer,  \n",
    "                    tra_loader=tra_loader,  \n",
    "                    val_loader=val_loader,  \n",
    "                    epoches=config.epoch,   \n",
    "                    lr_scheduler=lr_scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and metrics saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'unet_triple'\n",
    "# ## metrics saving\n",
    "# metrics_path = config.root + '/model/pretrained/' + model_name + '_metrics.csv'\n",
    "# metrics_df = pd.DataFrame(metrics)\n",
    "# metrics_df.to_csv(metrics_path, index=False, sep=',')\n",
    "# # metrics_df = pd.read_csv(metrics_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model save/load\n",
    "# model_weights = config.root + '/model/pretrained/' + model_name + '_weights.pth'\n",
    "# torch.save(model.state_dict(), model_weights)\n",
    "# model.load_state_dict(torch.load(model_weights))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f5bb23da6bd6ab87296804a7ae062a565b497c650c7064ca78191dd29b49fd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('venv-luo': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "6f5bb23da6bd6ab87296804a7ae062a565b497c650c7064ca78191dd29b49fd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}